{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import integration.llm \n",
    "import integration.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response.pprint_utils import pprint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 15:30:26--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_march_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1260185 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_march_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_m 100%[===================>]   1.20M  2.00MB/s    in 0.6s    \n",
      "\n",
      "2025-03-11 15:30:29 (2.00 MB/s) - ‘data/10q/uber_10q_march_2022.pdf’ saved [1260185/1260185]\n",
      "\n",
      "--2025-03-11 15:30:29--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_june_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1238483 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_june_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_j 100%[===================>]   1.18M   917KB/s    in 1.3s    \n",
      "\n",
      "2025-03-11 15:30:32 (917 KB/s) - ‘data/10q/uber_10q_june_2022.pdf’ saved [1238483/1238483]\n",
      "\n",
      "--2025-03-11 15:30:32--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_sept_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1178622 (1.1M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_sept_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_s 100%[===================>]   1.12M  3.59MB/s    in 0.3s    \n",
      "\n",
      "2025-03-11 15:30:33 (3.59 MB/s) - ‘data/10q/uber_10q_sept_2022.pdf’ saved [1178622/1178622]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10q/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_march_2022.pdf' -O 'data/10q/uber_10q_march_2022.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_june_2022.pdf' -O 'data/10q/uber_10q_june_2022.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_sept_2022.pdf' -O 'data/10q/uber_10q_sept_2022.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_march_2022.pdf\"]\n",
    ").load_data()\n",
    "june_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_june_2022.pdf\"]\n",
    ").load_data()\n",
    "sept_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_sept_2022.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_index = VectorStoreIndex.from_documents(march_2022)\n",
    "june_index = VectorStoreIndex.from_documents(june_2022)\n",
    "sept_index = VectorStoreIndex.from_documents(sept_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"qwen2:7b\",request_timeout=360.0)\n",
    "march_engine = march_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "june_engine = june_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "sept_engine = sept_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "query_tool_sept = QueryEngineTool.from_defaults(\n",
    "    query_engine=sept_engine,\n",
    "    name=\"sept_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending\"\n",
    "        f\" September 2022\"\n",
    "    ),\n",
    ")\n",
    "query_tool_june = QueryEngineTool.from_defaults(\n",
    "    query_engine=june_engine,\n",
    "    name=\"june_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending June\"\n",
    "        f\" 2022\"\n",
    "    ),\n",
    ")\n",
    "query_tool_march = QueryEngineTool.from_defaults(\n",
    "    query_engine=march_engine,\n",
    "    name=\"march_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending March\"\n",
    "        f\" 2022\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query plan tool\n",
    "from llama_index.core.tools import QueryPlanTool\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_plan_tool = QueryPlanTool.from_defaults(\n",
    "    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'query_plan_tool',\n",
       "  'description': '        This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n        Tool Name: sept_2022\\nTool Description: Provides information about Uber quarterly financials ending September 2022 \\n\\nTool Name: june_2022\\nTool Description: Provides information about Uber quarterly financials ending June 2022 \\n\\nTool Name: march_2022\\nTool Description: Provides information about Uber quarterly financials ending March 2022 \\n        ',\n",
       "  'parameters': {'$defs': {'QueryNode': {'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.',\n",
       "     'properties': {'id': {'description': 'ID of the query node.',\n",
       "       'title': 'Id',\n",
       "       'type': 'integer'},\n",
       "      'query_str': {'description': 'Question we are asking. This is the query string that will be executed. ',\n",
       "       'title': 'Query Str',\n",
       "       'type': 'string'},\n",
       "      'tool_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'default': None,\n",
       "       'description': 'Name of the tool to execute the `query_str`.',\n",
       "       'title': 'Tool Name'},\n",
       "      'dependencies': {'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.',\n",
       "       'items': {'type': 'integer'},\n",
       "       'title': 'Dependencies',\n",
       "       'type': 'array'}},\n",
       "     'required': ['id', 'query_str'],\n",
       "     'title': 'QueryNode',\n",
       "     'type': 'object'}},\n",
       "   'properties': {'nodes': {'description': 'The original question we are asking.',\n",
       "     'items': {'$ref': '#/$defs/QueryNode'},\n",
       "     'title': 'Nodes',\n",
       "     'type': 'array'}},\n",
       "   'required': ['nodes'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_plan_tool.metadata.to_openai_tool()  # to_openai_function() deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "llm must be a OpenAI instance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIAgent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[0;32m----> 4\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_plan_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_function_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/grinda/lib/python3.10/site-packages/llama_index/agent/openai/base.py:111\u001b[0m, in \u001b[0;36mOpenAIAgent.from_tools\u001b[0;34m(cls, tools, tool_retriever, llm, chat_history, memory, memory_cls, verbose, max_function_calls, default_tool_choice, callback_manager, system_prompt, prefix_messages, tool_call_parser, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, OpenAI):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm must be a OpenAI instance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "\u001b[0;31mValueError\u001b[0m: llm must be a OpenAI instance"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent import FnAgentWorker\n",
    "\n",
    "agent = FnAgentWorker(\n",
    "    fn=retry_agent_fn,\n",
    "    initial_state={\n",
    "        \"prompt_str\": DEFAULT_PROMPT_STR,\n",
    "        \"llm\": llm,\n",
    "        \"router_query_engine\": router_query_engine,\n",
    "        \"current_reasoning\": [],\n",
    "        \"verbose\": True,\n",
    "    },\n",
    ").as_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grinda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
